{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ca187e0c-b75f-4bfb-bc20-7869e7ab4d1a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import datasets \n",
    "from datasets import Features, Image, load_dataset, load_dataset_builder, config, concatenate_datasets\n",
    "# import matplotlib.pyplot as plt\n",
    "# import os\n",
    "# import shutil\n",
    "# import pathlib\n",
    "# import PIL\n",
    "# from PIL import Image\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "from torchvision.transforms import v2, ToTensor, Lambda\n",
    "from torchvision.io import read_image\n",
    "from torch.utils.data import Dataset, DataLoader, Subset\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0d0924a0-b4e9-4375-9897-2d6870819604",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d984c208b40248859c9709751410e055",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# get data (from https://huggingface.co/datasets/YakupAkdin/instrument-images/tree/main)\n",
    "dataset = load_dataset(\"YakupAkdin/instrument-images\", split='train').with_format(\"torch\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "67514ad2-0eb1-46ef-a69e-39013c33f034",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "989"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_RGB = dataset.filter(lambda x: x['image'].dim() == 3)\n",
    "len(dataset_RGB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e989a50e-88f7-4a13-94c1-6bc23b14ae1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([720, 1280, 3])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 720, 1280])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# the RGB images are [H,W,3] and PyTorch needs them as [3,H,W]\n",
    "# including [:3, :, :] to adjust any RGBA images to RGB\n",
    "print(dataset_RGB[0]['image'].shape)\n",
    "\n",
    "# permute the tensors\n",
    "def permute_images(batch):\n",
    "    # Apply the permute operation to each tensor in the batch\n",
    "    batch['image'] = [img.permute(2, 0, 1)[:3, :, :] for img in batch['image']]\n",
    "    return batch\n",
    "\n",
    "dataset_RGB = dataset_RGB.map(permute_images, batched=True, batch_size=100)\n",
    "\n",
    "dataset_RGB[0]['image'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b8cf5c48-7f8d-4cc1-a50a-a4af909037be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_greyscale = dataset.filter(lambda x: x['image'].dim() == 2)\n",
    "len(dataset_greyscale)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2dab59e8-c1fb-44ea-b8c2-ab04fbb48e80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([849, 900])\n",
      "torch.Size([3, 849, 900])\n"
     ]
    }
   ],
   "source": [
    "# some of the images are greyscale, so we convert to RGB as well\n",
    "print(dataset_greyscale[0]['image'].shape)\n",
    "\n",
    "def grayscale_to_rgb(item):\n",
    "    item['image'] = item['image'].unsqueeze(0).repeat(3, 1, 1)\n",
    "    return item\n",
    "\n",
    "dataset_greyscale = dataset_greyscale.map(grayscale_to_rgb)\n",
    "print(dataset_greyscale[0]['image'].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7d94e543-a904-42e5-904e-dc32b37e3ec8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# concatenate both reformatted RGB datasets\n",
    "dataset = concatenate_datasets([dataset_RGB, dataset_greyscale])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "74d2659d-c27f-486e-b617-b48174a1254a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample values: tensor([40, 39, 41, 41, 41, 44, 48, 53, 54, 52])\n",
      "Max value: 255\n",
      "Min value: 0\n"
     ]
    }
   ],
   "source": [
    "# confirming 0-255 scaling of image tensors\n",
    "sample_image = dataset[0]['image']\n",
    "\n",
    "print(\"Sample values:\", sample_image.flatten()[:10])\n",
    "print(\"Max value:\", sample_image.max().item())\n",
    "print(\"Min value:\", sample_image.min().item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fa100f3a-a360-41a1-9d56-b2c92d479c88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# rescale tensors from 0-255 to 0-1\n",
    "class ScaleTensor(object):\n",
    "    def __call__(self, tensor):\n",
    "        return tensor / 255.\n",
    "\n",
    "# compose transformation function -> this is a more standard transform (according to pytorch documentation) to use later. Fitting my transform to the CIFAR tutorial model for now\n",
    "# basic_transform = v2.Compose([\n",
    "#     ScaleTensor(),\n",
    "#     v2.RandomResizedCrop(size=(224, 224), antialias=True),\n",
    "#     v2.RandomHorizontalFlip(p=0.5),\n",
    "#     v2.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5]),\n",
    "# ])\n",
    "\n",
    "basic_transform = v2.Compose([\n",
    "    ScaleTensor(),\n",
    "    v2.Resize(32),\n",
    "    v2.CenterCrop(32),\n",
    "    v2.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5]),\n",
    "])\n",
    "\n",
    "# application function for transforms\n",
    "def apply_transform(example):\n",
    "    # Apply the transformation to the 'image' field\n",
    "    example['image'] = [basic_transform(img) for img in example['image']]\n",
    "    return example\n",
    "\n",
    "# apply transforms\n",
    "dataset = dataset.map(apply_transform, batched=True, batch_size=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "99caa3fb-9fcc-4f82-8f3c-1b07375b7be0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample values: tensor([-0.7207, -0.7665, -0.5314, -0.7465, -0.6850, -0.7010, -0.4317, -0.4633,\n",
      "        -0.1801, -0.5389])\n",
      "Max value: 0.8591049909591675\n",
      "Min value: -0.8577016592025757\n"
     ]
    }
   ],
   "source": [
    "# checking the first image tensor\n",
    "sample_image = dataset[0]['image']\n",
    "print(\"Sample values:\", sample_image.flatten()[:10])\n",
    "print(\"Max value:\", sample_image.max().item())\n",
    "print(\"Min value:\", sample_image.min().item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f49d3032-22bc-4077-914e-42f51392867f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split test/train\n",
    "dataset = dataset.train_test_split(test_size=0.2, seed=1, stratify_by_column=\"label\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8856b487-609d-4e03-8b55-c7c3ec85447e",
   "metadata": {},
   "source": [
    "### Resources\n",
    "\n",
    "https://huggingface.co/docs/datasets/use_with_pytorch\n",
    "https://huggingface.co/docs/datasets/v2.14.5/en/image_load\n",
    "\n",
    "**Process images:**\n",
    "- image processing: https://huggingface.co/docs/datasets/image_process#map\n",
    "- transforms methods: https://pytorch.org/vision/stable/transforms.html#transforms\n",
    "- HF general processing: https://huggingface.co/docs/datasets/process\n",
    "- transforms v2 reference: https://pytorch.org/vision/stable/auto_examples/transforms/plot_transforms_getting_started.html#sphx-glr-auto-examples-transforms-plot-transforms-getting-started-py\n",
    "\n",
    "**Test/Train split**\n",
    "- use this: https://huggingface.co/docs/datasets/v2.15.0/en/package_reference/main_classes#datasets.Dataset.train_test_split\n",
    "\n",
    "**Follow rest of tutorial**\n",
    "- https://pytorch.org/tutorials/beginner/blitz/cifar10_tutorial.html\n",
    "\n",
    "**Building a Model Basics**\n",
    "- https://pytorch.org/tutorials/beginner/basics/buildmodel_tutorial.html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "319d9633-d481-40c2-97c0-55c4b51ef2c5",
   "metadata": {},
   "source": [
    "### Dataloader Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "2d3bd9e7-ddc6-4734-a1df-fcd80b0c4a53",
   "metadata": {},
   "outputs": [],
   "source": [
    "class HF_to_PyTorch(Dataset):\n",
    "    def __init__(self, huggingface_dataset):\n",
    "        self.huggingface_dataset = huggingface_dataset\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.huggingface_dataset)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Assuming each item is a dictionary with 'image' and 'label' keys\n",
    "        item = self.huggingface_dataset[idx]\n",
    "        image = item['image']\n",
    "        label = item['label']\n",
    "        return image, label\n",
    "\n",
    "train_dataset = HF_to_PyTorch(dataset['train'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "17d5662d-0f2f-4019-ba1c-48cd83d2daa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainloader = DataLoader(train_dataset, batch_size=80, shuffle=True, num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "51f7075d-b5e8-4ec2-a104-822b79a128e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# trainloader = DataLoader(dataset['train'], batch_size=64, shuffle=True)\n",
    "# testloader = DataLoader(dataset['test'], batch_size=4, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "5e0b4194-c2c2-43d8-a996-bb1e73f682bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# start with basic model from the tutorial\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 6, 5)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
    "        self.fc1 = nn.Linear(16 * 5 * 5, 120)\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = torch.flatten(x, 1) # flatten all dimensions except batch\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "net = Net()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "63daa908-5f77-483e-995a-6c536f1c9244",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define loss function: Classification Cross-Entropy loss and SGD with momentum.\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "bde8f251-aef4-482d-a6d6-e0fd440deca7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1,     1] loss: 0.001\n",
      "[1,     2] loss: 0.001\n",
      "[1,     3] loss: 0.001\n",
      "[1,     4] loss: 0.001\n",
      "[1,     5] loss: 0.001\n",
      "[1,     6] loss: 0.001\n",
      "[1,     7] loss: 0.001\n",
      "[1,     8] loss: 0.001\n",
      "[1,     9] loss: 0.001\n",
      "[1,    10] loss: 0.001\n",
      "[2,     1] loss: 0.001\n",
      "[2,     2] loss: 0.001\n",
      "[2,     3] loss: 0.001\n",
      "[2,     4] loss: 0.001\n",
      "[2,     5] loss: 0.001\n",
      "[2,     6] loss: 0.001\n",
      "[2,     7] loss: 0.001\n",
      "[2,     8] loss: 0.001\n",
      "[2,     9] loss: 0.001\n",
      "[2,    10] loss: 0.001\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(2):  # loop over the dataset multiple times\n",
    "\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(trainloader, 0):\n",
    "        # get the inputs; data is a list of [inputs, labels]\n",
    "        inputs, labels = data\n",
    "\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        outputs = net(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # print statistics\n",
    "        running_loss += loss.item()\n",
    "        # if i % 2000 == 1999:    # print every 2000 mini-batches\n",
    "        print(f'[{epoch + 1}, {i + 1:5d}] loss: {running_loss / 2000:.3f}')\n",
    "        running_loss = 0.0\n",
    "\n",
    "print('Finished Training')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
