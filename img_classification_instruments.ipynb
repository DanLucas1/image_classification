{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ca187e0c-b75f-4bfb-bc20-7869e7ab4d1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Matplotlib created a temporary cache directory at /tmp/matplotlib-swysp0s3 because the default path (/.config/matplotlib) is not a writable directory; it is highly recommended to set the MPLCONFIGDIR environment variable to a writable directory, in particular to speed up the import of Matplotlib and to better support multiprocessing.\n"
     ]
    }
   ],
   "source": [
    "# import libraries\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import shutil\n",
    "import pathlib\n",
    "import PIL\n",
    "from PIL import Image\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "from torchvision.transforms import v2, ToTensor, Lambda\n",
    "from torchvision.io import read_image\n",
    "from torch.utils.data import Dataset, DataLoader, Subset\n",
    "\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7c53afb3-f04b-48b8-86d3-bb20539d4cd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a labels dictionary\n",
    "img_dir = os.path.join(os.getcwd(), 'instrument-images')\n",
    "\n",
    "labels = {\n",
    "    'acoustic_guitar': 0,\n",
    "    'baglama': 1,\n",
    "    'electric_guitar': 2,\n",
    "    'harp': 3,\n",
    "    'kanun': 4,\n",
    "    'kemenche': 5,\n",
    "    'mandolin': 6,\n",
    "    'oud': 7,\n",
    "    'violin': 8,\n",
    "    'yayli_tambur': 9}\n",
    "\n",
    "inverted_labels = {v: k for k, v in labels.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "56695d8b-08c3-4cde-a45c-68b276be1704",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# ensure all images are in 8-bit RBG format\n",
    "def convert_to_rgb(img_path):\n",
    "    with Image.open(img_path) as img:\n",
    "        if img.mode != 'RGB':\n",
    "            img = img.convert('RGB')\n",
    "        img.save(img_path)\n",
    "\n",
    "for filename in os.listdir(img_dir):\n",
    "    if filename.lower().endswith((\".jpg\", \".jpeg\", \".png\", \".gif\")):\n",
    "        img_path = os.path.join(img_dir, filename)\n",
    "        convert_to_rgb(img_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "bf0aa337-2f7f-421a-904b-64bc9225a975",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label\n",
      "0    100\n",
      "1    100\n",
      "2    100\n",
      "3    100\n",
      "4    100\n",
      "5    100\n",
      "6    100\n",
      "7    100\n",
      "8    100\n",
      "9    100\n",
      "Name: count, dtype: int64\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>acoustic_guitar_95.jpg</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>acoustic_guitar_96.jpg</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>acoustic_guitar_97.jpg</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>acoustic_guitar_98.jpg</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>acoustic_guitar_99.jpg</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>baglama_1.jpg</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101</th>\n",
       "      <td>baglama_10.jpg</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102</th>\n",
       "      <td>baglama_100.jpg</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103</th>\n",
       "      <td>baglama_11.jpg</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104</th>\n",
       "      <td>baglama_12.jpg</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      image  label\n",
       "95   acoustic_guitar_95.jpg      0\n",
       "96   acoustic_guitar_96.jpg      0\n",
       "97   acoustic_guitar_97.jpg      0\n",
       "98   acoustic_guitar_98.jpg      0\n",
       "99   acoustic_guitar_99.jpg      0\n",
       "100           baglama_1.jpg      1\n",
       "101          baglama_10.jpg      1\n",
       "102         baglama_100.jpg      1\n",
       "103          baglama_11.jpg      1\n",
       "104          baglama_12.jpg      1"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# read in image labels\n",
    "image_labels = pd.read_csv(os.path.join(img_dir, 'image_labels.csv'))\n",
    "print(image_labels.label.value_counts()) # confirm 100 of each label\n",
    "image_labels.iloc[95:105] # check a \"transition\" section of the df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e909bf3-ce1c-4efa-ab85-9b1f4b799f00",
   "metadata": {},
   "source": [
    "### Build Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "36d31804-cac5-4e5b-8d5e-1acc3df58d03",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImgClassDataset(Dataset):\n",
    "    def __init__(self, annotations_file, img_dir, transform=None, target_transform=None):\n",
    "        self.img_labels = pd.read_csv(annotations_file)\n",
    "        self.img_dir = img_dir\n",
    "        self.transform = transform\n",
    "        self.target_transform = target_transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = os.path.join(self.img_dir, self.img_labels.iloc[idx, 0])\n",
    "        image = read_image(img_path)\n",
    "        label = self.img_labels.iloc[idx, 1]\n",
    "        if self.transform:\n",
    "            image=self.transform(image)\n",
    "        if self.target_transform:\n",
    "            label=self.target_transform(label)\n",
    "        return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "599b7fda-1a94-4a15-919b-f23647bc3b56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# input image transforms\n",
    "\n",
    "torch.manual_seed(1)\n",
    "\n",
    "img_transform = v2.Compose([\n",
    "    v2.RandomResizedCrop(size=(224, 224), antialias=True),\n",
    "    v2.RandomHorizontalFlip(p=0.5),\n",
    "    # v2.RandomVerticalFlip(p=0.5),\n",
    "    v2.ToDtype(torch.float32, scale=True),\n",
    "    v2.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "de6d1a40-c1e2-455e-ac22-8e2f073a228e",
   "metadata": {},
   "outputs": [],
   "source": [
    "instrument_images = ImgClassDataset(\n",
    "    os.path.join(img_dir, 'image_labels.csv'),\n",
    "    img_dir,\n",
    "    img_transform,\n",
    "    Lambda(lambda y: torch.zeros(10, dtype=torch.float).scatter_(0, torch.tensor(y), value=1))) # one hot encoded tensor output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ad02282e-02bd-45fe-a27f-811159d429a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 224, 224])\n",
      "tensor([1., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n"
     ]
    }
   ],
   "source": [
    "# check size of a data element and print label\n",
    "print(instrument_images[0][0].shape)\n",
    "print(instrument_images[0][1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9af9d0d-dda2-4ba9-b94c-d4c4b045b453",
   "metadata": {},
   "source": [
    "### Test/Train Splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8c3d658e-7949-4f38-84ec-a1a2b5066003",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# get index lists for training and testing data\n",
    "# data_labels = [instrument_images[i][1] for i in range(len(instrument_images))]\n",
    "data_labels = image_labels.label # faster to just use this list\n",
    "\n",
    "train, test = train_test_split(\n",
    "    range(len(instrument_images)),\n",
    "    test_size=0.2, \n",
    "    stratify=data_labels,\n",
    "    random_state = 1)\n",
    "\n",
    "# subset full dataset according to indices\n",
    "training_data = Subset(instrument_images, train)\n",
    "test_data = Subset(instrument_images, test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21de29e4-16ad-4ce3-bbc0-4f81e45e2a13",
   "metadata": {},
   "source": [
    "### Dataloader setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "076dfb58-7c3b-487f-8d58-158ddd8b8a73",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(training_data, batch_size=64, shuffle=True)\n",
    "test_dataloader = DataLoader(test_data, batch_size=64, shuffle=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
