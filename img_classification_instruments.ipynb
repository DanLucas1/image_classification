{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca187e0c-b75f-4bfb-bc20-7869e7ab4d1a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import datasets \n",
    "from datasets import Features, Image, load_dataset, load_dataset_builder, config, concatenate_datasets\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "# import shutil\n",
    "# import pathlib\n",
    "# import PIL\n",
    "# from PIL import Image\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "from torchvision.transforms import v2, ToTensor, Lambda\n",
    "from torchvision.io import read_image\n",
    "from torch.utils.data import Dataset, DataLoader, Subset\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d0924a0-b4e9-4375-9897-2d6870819604",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get data (from https://huggingface.co/datasets/YakupAkdin/instrument-images/tree/main)\n",
    "dataset = load_dataset(\"YakupAkdin/instrument-images\", split='train').with_format(\"torch\")\n",
    "\n",
    "label_values = {\n",
    "    0: 'harp',\n",
    "    1: 'baglama',\n",
    "    2: 'electric guitar',\n",
    "    3: 'acoustic guitar',\n",
    "    4: 'kanun',\n",
    "    5: 'violin',\n",
    "    6: 'kemence',\n",
    "    7: 'mandolin',\n",
    "    8: 'oud',\n",
    "    9: 'yayli tambur'}\n",
    "\n",
    "inverted_labels = {v: k for k, v in label_values.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67514ad2-0eb1-46ef-a69e-39013c33f034",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_RGB = dataset.filter(lambda x: x['image'].dim() == 3)\n",
    "len(dataset_RGB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e989a50e-88f7-4a13-94c1-6bc23b14ae1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the RGB images are [H,W,3] and PyTorch needs them as [3,H,W]\n",
    "# included [:3, :, :] slice to adjust any RGBA images to RGB\n",
    "print(dataset_RGB[0]['image'].shape)\n",
    "\n",
    "# permute the tensors\n",
    "def permute_images(batch):\n",
    "    # Apply the permute operation to each tensor in the batch\n",
    "    batch['image'] = [img.permute(2, 0, 1)[:3, :, :] for img in batch['image']]\n",
    "    return batch\n",
    "\n",
    "dataset_RGB = dataset_RGB.map(permute_images, batched=True, batch_size=100)\n",
    "\n",
    "dataset_RGB[0]['image'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8cf5c48-7f8d-4cc1-a50a-a4af909037be",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_greyscale = dataset.filter(lambda x: x['image'].dim() == 2)\n",
    "len(dataset_greyscale)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dab59e8-c1fb-44ea-b8c2-ab04fbb48e80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# some of the images are greyscale, so we convert to RGB as well\n",
    "print(dataset_greyscale[0]['image'].shape)\n",
    "\n",
    "def grayscale_to_rgb(item):\n",
    "    item['image'] = item['image'].unsqueeze(0).repeat(3, 1, 1)\n",
    "    return item\n",
    "\n",
    "dataset_greyscale = dataset_greyscale.map(grayscale_to_rgb)\n",
    "print(dataset_greyscale[0]['image'].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d94e543-a904-42e5-904e-dc32b37e3ec8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# concatenate both reformatted RGB datasets\n",
    "dataset = concatenate_datasets([dataset_RGB, dataset_greyscale])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74d2659d-c27f-486e-b617-b48174a1254a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# confirming 0-255 scaling of image tensors\n",
    "sample_image = dataset[0]['image']\n",
    "\n",
    "print(\"Sample values:\", sample_image.flatten()[:10])\n",
    "print(\"Max value:\", sample_image.max().item())\n",
    "print(\"Min value:\", sample_image.min().item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa100f3a-a360-41a1-9d56-b2c92d479c88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# rescale tensors from 0-255 to 0-1\n",
    "class ScaleTensor(object):\n",
    "    def __call__(self, tensor):\n",
    "        return tensor / 255.\n",
    "\n",
    "# compose transformation function -> this is a more standard transform (according to pytorch documentation) to use later. Fitting my transform to the CIFAR tutorial model for now\n",
    "# basic_transform = v2.Compose([\n",
    "#     ScaleTensor(),\n",
    "#     v2.RandomResizedCrop(size=(224, 224), antialias=True),\n",
    "#     v2.RandomHorizontalFlip(p=0.5),\n",
    "#     v2.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5]),\n",
    "# ])\n",
    "\n",
    "basic_transform = v2.Compose([\n",
    "    ScaleTensor(),\n",
    "    v2.Resize(32),\n",
    "    v2.CenterCrop(32),\n",
    "    v2.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5]),\n",
    "])\n",
    "\n",
    "# application function for transforms\n",
    "def apply_transform(example):\n",
    "    # Apply the transformation to the 'image' field\n",
    "    example['image'] = [basic_transform(img) for img in example['image']]\n",
    "    return example\n",
    "\n",
    "# apply transforms\n",
    "dataset = dataset.map(apply_transform, batched=True, batch_size=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fbdbf51-519a-4177-a8a3-d081954e50fa",
   "metadata": {},
   "source": [
    "### Examine Dataset/Model Compatibility Before Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99caa3fb-9fcc-4f82-8f3c-1b07375b7be0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# are tensor values normalized to [-1, 1]?\n",
    "\n",
    "correct_tensor_range = 0\n",
    "\n",
    "for i, image in enumerate(dataset['image']):\n",
    "    if image.min() < -1 or image.max() > 1:\n",
    "        print(f\"Image at index {i} has values outside expected range: min={image.min()}, max={image.max()}\")\n",
    "    else:\n",
    "        correct_tensor_range += 1\n",
    "\n",
    "print(correct_tensor_range)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8cc3ee7-862d-4a85-888f-013c37ce9ef7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# are all tensors shape [3, 32, 32]?\n",
    "\n",
    "correct_tensor_shape = 0\n",
    "\n",
    "for i in range(len(dataset)):\n",
    "    if dataset[i]['image'].shape != torch.Size([3, 32, 32]):  # replace with expected shape\n",
    "        print(f\"Image at index {i} has incorrect shape: {image.shape}\")\n",
    "    else:\n",
    "        correct_tensor_shape += 1\n",
    "\n",
    "print(correct_tensor_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4e6319b-746d-455e-882e-b95622d1ffb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking images and labels for consistency\n",
    "def imshow(img):\n",
    "    img = img / 2 + 0.5     # unnormalize\n",
    "    npimg = img.numpy()\n",
    "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
    "    plt.show()\n",
    "\n",
    "# get some random training images\n",
    "def check_instrument(instr, ds):\n",
    "\n",
    "    check = ds.filter(lambda x: x['label'] == instr)\n",
    "    images = check['image'][0:32]\n",
    "    labels = check['label'][0:32]\n",
    "\n",
    "    # show images & labels\n",
    "    imshow(torchvision.utils.make_grid(images))\n",
    "\n",
    "    label_list = ''\n",
    "    for i, lbl in enumerate(labels):\n",
    "        label_list += label_values[labels[i].item()]\n",
    "        if i % 8 == 7:\n",
    "            label_list += '\\n'\n",
    "        else:\n",
    "            label_list += ' | '\n",
    "    print(label_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dae88ab1-bcb8-42b7-ab5e-05614d519a84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# should be all violins\n",
    "check_instrument(5, dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba851469-dd1e-4299-9b68-196b23887ee4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# should be all electric guitars\n",
    "check_instrument(2, dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8856b487-609d-4e03-8b55-c7c3ec85447e",
   "metadata": {},
   "source": [
    "### Resources\n",
    "\n",
    "https://huggingface.co/docs/datasets/use_with_pytorch\n",
    "https://huggingface.co/docs/datasets/v2.14.5/en/image_load\n",
    "\n",
    "**Process images:**\n",
    "- image processing: https://huggingface.co/docs/datasets/image_process#map\n",
    "- transforms methods: https://pytorch.org/vision/stable/transforms.html#transforms\n",
    "- HF general processing: https://huggingface.co/docs/datasets/process\n",
    "- transforms v2 reference: https://pytorch.org/vision/stable/auto_examples/transforms/plot_transforms_getting_started.html#sphx-glr-auto-examples-transforms-plot-transforms-getting-started-py\n",
    "\n",
    "**Test/Train split**\n",
    "- use this: https://huggingface.co/docs/datasets/v2.15.0/en/package_reference/main_classes#datasets.Dataset.train_test_split\n",
    "\n",
    "**Follow rest of tutorial**\n",
    "- https://pytorch.org/tutorials/beginner/blitz/cifar10_tutorial.html\n",
    "\n",
    "**Building a Model Basics**\n",
    "- https://pytorch.org/tutorials/beginner/basics/buildmodel_tutorial.html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "319d9633-d481-40c2-97c0-55c4b51ef2c5",
   "metadata": {},
   "source": [
    "### Dataloader Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "359e9dd6-6b9d-405f-b0be-98ec9a7c06e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split test/train\n",
    "dataset = dataset.train_test_split(test_size=0.2, seed=1, stratify_by_column=\"label\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51f7075d-b5e8-4ec2-a104-822b79a128e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 4\n",
    "\n",
    "trainloader = DataLoader(dataset['train'], batch_size=batch_size, shuffle=True, num_workers=2)\n",
    "testloader = DataLoader(dataset['test'], batch_size=batch_size, shuffle=True, num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d71bb018-1bbc-4cb7-9f12-37a77a0be7fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check batch sizing (should be no output)\n",
    "for batch in trainloader:\n",
    "    if batch['image'].shape[0] != batch_size:\n",
    "        print(f\"Batch {i} has incorrect batch size: {images.shape[0]}\")\n",
    "    if batch['label'].shape[0] != batch_size:\n",
    "        print(f\"Batch {i} has mismatched images and labels\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e0b4194-c2c2-43d8-a996-bb1e73f682bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# start with basic model from the tutorial\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 6, 5)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
    "        self.fc1 = nn.Linear(16 * 5 * 5, 120)\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = torch.flatten(x, 1) # flatten all dimensions except batch\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "net = Net()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63daa908-5f77-483e-995a-6c536f1c9244",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define loss function: Classification Cross-Entropy loss and SGD with momentum.\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d08d4928-117c-48a8-8044-0eecd2d92a15",
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(25):  # loop over the dataset multiple times\n",
    "\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(trainloader, 0):\n",
    "        inputs = data['image']\n",
    "        labels = data['label']\n",
    "        \n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        outputs = net(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # print statistics\n",
    "        running_loss += loss.item()\n",
    "        if i % 200 == 199:    # print every 200 mini-batches\n",
    "            print(f'[{epoch + 1}, {i + 1:5d}] loss: {running_loss / 200:.3f}')\n",
    "            running_loss = 0.0\n",
    "\n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e602dbdb-7db0-4885-b83f-07afd6e62716",
   "metadata": {},
   "source": [
    "### Saving and Testing the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc46fc1c-861f-4b6f-a36d-489a8b1a4e5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # save the locally-trained model in current working directory\n",
    "pwd = os.getcwd()\n",
    "filename = 'model v001.pth'\n",
    "path = os.path.join(pwd, filename)\n",
    "torch.save(net.state_dict(), path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5fec3a6-dcf1-4383-98d7-202bd8f28611",
   "metadata": {},
   "source": [
    "### Check Model Performance on Test Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cfb0f83-49a5-4c21-98ac-8e06c45e58ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# in case we need to load model later\n",
    "# pwd = os.getcwd()\n",
    "# filename = 'model v001.pth'\n",
    "# path = os.path.join(pwd, filename)\n",
    "# net.load_state_dict(torch.load(path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "609847e2-489d-46f1-ab4a-9aa86a798929",
   "metadata": {},
   "outputs": [],
   "source": [
    "## testing a batch of images\n",
    "\n",
    "# iterate through the dataset\n",
    "dataiter = iter(testloader)\n",
    "batch = next(dataiter)\n",
    "images = batch['image']\n",
    "labels = batch['label']\n",
    "\n",
    "# print some test imagery and labels\n",
    "imshow(torchvision.utils.make_grid(images))\n",
    "actual = [label_values[labels[i].item()] for i in range(4)]\n",
    "print('actual: '+', '.join(actual))\n",
    "\n",
    "# test the model output on these images\n",
    "outputs = net(images)\n",
    "predicted = [label_values[i.item()] for i in torch.max(outputs, 1)[1]]\n",
    "print('predicted: '+', '.join(predicted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6cf614a-0459-4d78-9ec6-0c1d90cb29fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate accuracy\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "for data in testloader:\n",
    "    images = data['image']\n",
    "    labels = data['label']\n",
    "\n",
    "    prediction = net(images)\n",
    "    predicted = torch.max(prediction, 1)[1]\n",
    "    total += labels.size(0)\n",
    "    correct += (predicted == labels).sum().item()\n",
    "\n",
    "print(f'{correct} of {total} images categorized correctly: {100 * correct // total} %')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
